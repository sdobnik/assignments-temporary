{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qgl75QHLfvFi"
   },
   "source": [
    "# A2: Vector Semantics\n",
    "\n",
    "By Nikolai Ilinykh, Mehdi Ghanimifard, Wafia Adouane and Simon Dobnik. Updated in 2025 by Ricardo Muñoz Sánchez\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "In this lab we will look at how to build distributional semantic models from corpora and use semantic similarity captured by these models to do semantic tasks. We are also going to examine how different vector composition functions for vectors work in approximating semantic similarity of phrases when compared to human judgements.\n",
    "\n",
    "This lab uses code from a file called `dist_erk.py` which contains functions similar to those shown in the lecture. You can use either set of functions to solve these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "msoZfc7sfvFk",
    "outputId": "d0c13734-2ef5-46c0-81ff-560e2569b12f"
   },
   "outputs": [],
   "source": [
    "# The code for dist_erk.py uses both Spacy and NLTK, so make sure to have them installed!\n",
    "# Our code also uses SciPY and scikit-learn, so you'll need to install it as well.\n",
    "# If you're unsure how to do this, check out these websites:\n",
    "### https://scipy.org/beginner-install/\n",
    "### https://scikit-learn.org/stable/install.html\n",
    "### https://spacy.io/usage\n",
    "### https://www.nltk.org/install.html\n",
    "\n",
    "\n",
    "# We also need to make sure we have the necessary models and datasets for Spacy\n",
    "import spacy\n",
    "#spacy.cli.download('en_core_web_sm')\n",
    "\n",
    "# You only need to run this cell once\n",
    "# You *need* to restart the kernel after downloading the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a7ZPXMptfvFl"
   },
   "outputs": [],
   "source": [
    "# the following command simply imports all the methods from the dist_erk file\n",
    "from dist_erk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWbuX2ZOfvFl"
   },
   "source": [
    "## 1. Loading a corpus\n",
    "\n",
    "To train a distributional model, we first need a sufficiently large collection of texts which contain different words used frequently enough in different contexts. Here we will use a section of the Wikipedia corpus `wikipedia.txt` stored in `wikipedia.zip`. This file has been borrowed from another lab by [Richard Johansson](http://www.cse.chalmers.se/~richajo/).\n",
    "\n",
    "When unpacked, the file is 151mb, hence if you are using the MLT servers you should store it in a temporary folder outside your home and adjust the `corpus_dir` path below. It may already exist in `/srv/data/computational-semantics/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hQ2BMCQwfvFl"
   },
   "outputs": [],
   "source": [
    "corpus_dir = 'wikipedia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWBHZOmbfvFm"
   },
   "source": [
    "## 2. Building a model\n",
    "\n",
    "Now you are ready to build the model.  \n",
    "Using the methods from the code imported above build three word matrices with 1000 dimensions as follows:  \n",
    "\n",
    "(i) with raw counts (saved to a variable `space_1k`);  \n",
    "(ii) with PPMI (`ppmispace_1k`);  \n",
    "(iii) with reduced dimensions SVD (`svdspace_1k`).  \n",
    "For the latter use `svddim=5`. **[5 marks]**\n",
    "\n",
    "Your task is to replace `...` with function calls to functions from `dist_erk.py` which are similar to functions shown during the lecture.\n",
    "\n",
    "Do not despair if the code takes a bit long to run!\n",
    "It took me about 9 minutes for the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FgckHkcqfvFm",
    "outputId": "d6723de4-4f2a-4ff9-d574-46af33b75794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file wikipedia.txt\n",
      "create count matrices\n",
      "reading file wikipedia.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1145485it [01:40, 11427.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi transform\n",
      "svd transform\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "numdims = 1000\n",
    "svddim = 5\n",
    "\n",
    "# Which words to use as targets and context words?\n",
    "# We need to count the words and keep only the N most frequent ones\n",
    "# Which function would you use here with which variable?\n",
    "ktw = do_word_count(corpus_dir, numdims)\n",
    "\n",
    "wi = make_word_index(ktw) # word index\n",
    "words_in_order = sorted(wi.keys(), key=lambda w:wi[w]) # sorted words\n",
    "\n",
    "# Create different spaces (the original matrix space, the ppmi space, the svd space)\n",
    "# Which functions with which arguments would you use here?\n",
    "print('create count matrices')\n",
    "space_1k = make_space(corpus_dir, wi, numdims)\n",
    "print('ppmi transform')\n",
    "ppmispace_1k = ppmi_transform(space_1k, wi)\n",
    "print('svd transform')\n",
    "svdspace_1k = svd_transform(ppmispace_1k, numdims, svddim)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cohTlVz6fvFm"
   },
   "source": [
    "Comment :\n",
    "\n",
    "For words_in_order we use the sorted method but its may not be needed as the function do_word_count already sorts the words using the method most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XIwvkBN1fvFm",
    "outputId": "6d53294d-77fa-41f4-8303-79e0d2cbec93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2551 3714 3104  567  962  627  443  185  311  189  131   28   93  169\n",
      "   81  125  151  408  194   89   79   29  217  184   62   15   31   70\n",
      "   10    1   41   21    1   31   37    1   30    5   25    7    3   20\n",
      "   11    1   32   36    2    5   65    4    0   46    8   18   28    0\n",
      "   20    7    8   16   10   40    0  175   10    2    7   19    1  174\n",
      "   11    3    1    6    0    0    0   10    9   11    7   24    4    4\n",
      "   14   23   58    7    0   10    2    3   10    6   18    6   13    3\n",
      "   22    0    3    5    3    7   14    3   40   20   19   15    6    8\n",
      "   23    4    5    1   19    0    3    1    0   14    0   14   53    7\n",
      "    7   11    6    5    5    4   12    6   53    1    1  433    4    0\n",
      "    5    7    7   12    1    1    3    4   17    8   16    1    2   31\n",
      "    1   12   14    1   44    6   14    9   38    7    2    6    8    1\n",
      "   10    6   10    1    9    7    9    4    3    9    0   11    3    2\n",
      "    0    2   11   37    2    0    2    1    5    9   10   16    4    6\n",
      "    0   21    1    1    0    2   47    3   27    7    0    2   13    1\n",
      "    2    0    5   31    0    1    0    3    9    0    1    0    3    3\n",
      "   17    1    1   16    3    7    4    7   15    4    0    0    2    5\n",
      "    0    2    0    5    0    9    0    0    8    0   10    0    0    0\n",
      "    2    0    1    3    1    3   15    1    9    0   19   14    0    0\n",
      "    3    2   18    3    1    3    2   19    5    2    4    1   10    6\n",
      "    0    3    3    6    4    2   25    4    6    3    1   25   10   15\n",
      "    3   10   15    1   10    1    8    1   13    1    2    9    9    1\n",
      "    4    1   25    0    4    6    5    5   36    0    2    2    2    0\n",
      "    0    2    3    3    0    1    4    6    5    0   50    2    5    2\n",
      "   14    6    2    2    4    1    9    4    5    3    1    0   12    3\n",
      "    3    2    2    0    0    1    4    7   12    5    0    2    1    2\n",
      "    3    4    7    3    5    0   27    7    1    1    0    3    3    3\n",
      "   10    0   14    2    0    2    4    6    0    5    0    0    1    1\n",
      "    4    1    1    0    0    0    0    3   20    0    0    2    1    5\n",
      "    3    8    3    5    1    2   66    1    2   19    2    1    3    3\n",
      "   21    5    4    2    2    0    4    3    5    0    7    1    6    1\n",
      "    3    3    1    0    3    0    2    0   89    2    3    1    1   14\n",
      "    0    2    1    9    2    3    2    4    2    0   25    0    0   23\n",
      "    0    6    2    1    3    0    2    5    0    4    4    3    0    4\n",
      "   58    3    1    6    2    4    3    3   11    1    1    1   10    0\n",
      "    7    3    1    6    1   18    1    0    4    2    0    8    5    2\n",
      "    0    0    0    0    5    1    2    1    1    3    1    2    1    1\n",
      "    0    6    1    4    1    3   20    1    0    5    2    5    2    1\n",
      "    0    0    0    2    6    1    1    0    1    1    1    0    0    3\n",
      "    3    0    0    6    6   74    3    0   13    5    2    2    1    5\n",
      "    3    3    1    7    4    0    0    2    3    0    4    0    4    1\n",
      "    0    2    5    2    1   14    2    0    0   19    0    1    2    1\n",
      "    0    3    2    0    0    3    1    3    3    2    7   18    7    6\n",
      "    6    0    1    9    1   10    2    0    2    0    2    4    0    0\n",
      "    1    2    0    1    0    2    0    0    0    2    0    2    2    0\n",
      "    3    2    2    0    0    1    2    3    1    1    1    2    0    0\n",
      "    3    0    7    2   39    0   14    0    1    1    0    1    5    3\n",
      "   11    0    3    0    1    1    0    0    1    9    2    1    0   11\n",
      "    1    3    7    0    0    0   32    1    0    0    0    1    1    3\n",
      "    0    9    0    2    0    1    3    2    6    0    3    0    0    2\n",
      "    3    0    1    0    1    4    0    0    1    1    0    0    5   21\n",
      "    2    1    1    3    0    1    7    1    3    4    0    5    3    0\n",
      "    7    2    0    4    2    0    2    1    4    4    0    0    0    5\n",
      "    3    2    2    0    4    0   23    2    2    2    4    0    1    0\n",
      "    4    0    3    5    3    0    8    0    1   16    1    2    2    7\n",
      "    0    0    1   11    1    0    4    0    1    0    1    2    1    5\n",
      "    0   97    0    2    0    3    0    8    1   14    4    9    2    3\n",
      "    1    1    0    3    4    0    5    1    5    2    0    0    0    2\n",
      "    1    2    1    1    1    1   12    0    2    5    1    0    0   13\n",
      "    2    0    0    0    2    2    0    0    3    1    1    1    1    0\n",
      "    1    2    1    0    0    0   10    0    1    0    1    1    1    1\n",
      "    0    1    0    0    3    2    5    0    0    2    1    0   23    0\n",
      "    0    4    0    1    0    0    0    1    1    2    1    0    1    0\n",
      "    0    4    1    0    1    1    5    1    1    0    1    0    0    0\n",
      "    1    0    0    2    2    3    0    1    0    4    3    3    1    4\n",
      "    0    0    0    6    1    2    1    0    5    3    0    0    1    2\n",
      "    0    5    0    0    2    1    1    4   15    0    0    1    1    3\n",
      "    1    0    1    4    1    1    2    8    1    3    0    0    0    0\n",
      "    1    3    2    1    0    1    0    2    0    0    0    0    1    1\n",
      "    0    1    3    7    0    0   42    4    0    1    2    3    1    0\n",
      "    1    3    2    0    0    1    0    0    0    4    2    0    0    8\n",
      "    2    0    1   15    0    0]\n"
     ]
    }
   ],
   "source": [
    "# now, to test the space, you can print vector representation for some words\n",
    "print('house:', space_1k['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d4M63EzfvFm"
   },
   "source": [
    "Oxford Advanced Dictionary has 185,000 words, hence 1,000 words is not representative. We trained a model with 10,000 words, and 50 dimensions on truncated SVD. All matrices are available in the folder `pretrained` of the `wikipedia.zip`file. These are `ktw_wikipediaktw.npy`, `raw_wikipediaktw.npy`, `ppmi_wikipediaktw.npy`, `svd50_wikipedia10k.npy`. Make sure they are in your path as we load them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PJA-nbFdfvFm",
    "outputId": "b52fc165-fe45-4f76-c4a4-88498a1ab709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numdims = 10000\n",
    "svddim = 50\n",
    "\n",
    "print('Please wait...')\n",
    "ktw_10k       = np.load('wikipedia/pretrained/ktw_wikipediaktw.npy', allow_pickle=True)\n",
    "space_10k     = np.load('wikipedia/pretrained/raw_wikipediaktw.npy', allow_pickle=True).tolist()\n",
    "ppmispace_10k = np.load('wikipedia/pretrained/ppmi_wikipediaktw.npy', allow_pickle=True).tolist()\n",
    "svdspace_10k  = np.load('wikipedia/pretrained/svd50_wikipedia10k.npy', allow_pickle=True).tolist()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hjvmhIKDfvFn",
    "outputId": "ad0c0d4a-36ef-421b-9cd6-73eafd4145f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2554 3774 3105 ...    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# testing semantic space\n",
    "print('house:', space_10k['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-Rgd6dOfvFn"
   },
   "source": [
    "## 3. Testing semantic similarity\n",
    "\n",
    "The file `similarity_judgements.txt` contains 7,576 pairs of words and their lexical and visual similarities (based on the pictures) collected through crowd-sourcing using Mechanical Turk as described in [1]. The scores range from 1 (highly dissimilar) to 5 (highly similar). Note: this is a different dataset from the phrase similarity dataset we discussed during the lecture [2]. You can find more details about how they were collected in the papers.\n",
    "\n",
    "The following code will transform similarity scores into a Python-friendly format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QV9igs8FfvFn",
    "outputId": "39db4d06-e1ea-4370-8251-a2326da837e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available words to test: 12\n",
      "number of available word pairs to test: 774\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [] # test suit word pairs\n",
    "semantic_similarity = []\n",
    "visual_similarity = []\n",
    "test_vocab = set()\n",
    "\n",
    "for index, line in enumerate(open('similarity_judgements.txt')):\n",
    "    data = line.strip().split('\\t')\n",
    "    if index > 0 and len(data) == 3:\n",
    "        w1, w2 = tuple(data[0].split('#'))\n",
    "        # Checks if both words from each pair exist in the word matrix.\n",
    "        if w1 in ktw_10k and w2 in ktw_10k:\n",
    "            word_pairs.append((w1, w2))\n",
    "            test_vocab.update([w1, w2])\n",
    "            semantic_similarity.append(float(data[1]))\n",
    "            visual_similarity.append(float(data[2]))\n",
    "\n",
    "print('number of available words to test:', len(test_vocab-(test_vocab-set(ktw))))\n",
    "print('number of available word pairs to test:', len(word_pairs))\n",
    "#list(zip(word_pairs, visual_similarity, semantic_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acNE9cXJfvFn"
   },
   "source": [
    "We are going to test how the cosine similarity between vectors of each of the three spaces (normal space, ppmi, svd) compares with the human similarity judgements for the words in the similarity dataset. Which of the three spaces best approximates human judgements?\n",
    "\n",
    "For comparison of several scores, we can use [the Spearman correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) which is implemented in `scipy.stats.spearmanr` [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html). The values of the Sperman correlation coefficient range from -1, 0 to 1, where 0 indicates no correlation, 1 perfect correaltion and -1 negative correlation. Hence, the greater the number the better the similarity scores align. The p values tells us if the coefficient is statistically significant. For this to be the case, it must be less than or equal to $< 0.05$.\n",
    "\n",
    "Here is how you can calculate the Spearman correlation coefficient betweeen the scores of visual similarity and semantic similarity of the available words in the test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TITM1R2ifvFn",
    "outputId": "c4e71f06-2eea-43f4-ffbf-a38895abfea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs. Semantic Similarity:\n",
      "rho     = 0.7122\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, visual_similarity)\n",
    "print(\"\"\"Visual Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQjemkQAfvFn"
   },
   "source": [
    "Let's now calculate the cosine similarity scores of all word pairs in an ordered list using all three matrices. **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rerdj-NffvFn"
   },
   "outputs": [],
   "source": [
    "raw_similarities  = [cosine(w1, w2, space_10k) for w1, w2 in word_pairs]\n",
    "ppmi_similarities = [cosine(w1, w2, ppmispace_10k) for w1, w2 in word_pairs]\n",
    "svd_similarities  = [cosine(w1, w2, svdspace_10k) for w1, w2 in word_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83WikQcPfvFn"
   },
   "source": [
    "Calculate correlation coefficients between lists of similarity scores and the real semantic similarity scores from the experiment. The scores of what model best correlates them? Is this expected? **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4bCtq3-xfvFo",
    "outputId": "9bdf1bf3-9212-473f-95de-24047e820a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Similarity vs. Semantic Similarity:\n",
      "\trho     = 0.1522\n",
      "\tp-value = 0.0000\n",
      "PPMI Similarity vs. Semantic Similarity:\n",
      "\trho     = 0.4547\n",
      "\tp-value = 0.0000\n",
      "SVD Similarity vs. Semantic Similarity:\n",
      "\trho     = 0.4232\n",
      "\tp-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# your code should go here\n",
    "# for raw similarities\n",
    "rho_raw, pval_raw = stats.spearmanr(semantic_similarity, raw_similarities)\n",
    "print(\"\"\"Raw Similarity vs. Semantic Similarity:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_raw, pval_raw))\n",
    "# for PPMI similarities\n",
    "rho_ppmi, pval_ppmi = stats.spearmanr(semantic_similarity, ppmi_similarities)\n",
    "print(\"\"\"PPMI Similarity vs. Semantic Similarity:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_ppmi, pval_ppmi))\n",
    "# for SVD similarities\n",
    "rho_svd, pval_svd = stats.spearmanr(semantic_similarity, svd_similarities)\n",
    "print(\"\"\"SVD Similarity vs. Semantic Similarity:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_svd, pval_svd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhUkPF0zfvFo"
   },
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "The scores of the PPMI model best correlates with the real semantic similarity scores. The PPMI model is indeed expected to be better than the raw one because more information (mutual information) is extracted. SVD is a reduction from PPMI so some information is lost but SVD is supposed to be better when there is a lot of noise in the data. Maybe here, with only few data, the noise is already removed at the ppmi level, which could explain why the PPMI model performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVhQu--nfvFo"
   },
   "source": [
    "We can also calculate correlation coefficients between lists of cosine similarity scores and the real visual similarity scores from the experiment. Which similarity model best correlates with them? How do the correlation coefficients compare with those from the previous comparison - and can you speculate why do we get such results? **[7 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bSHrizkafvFo",
    "outputId": "bacc3668-3cd9-4d0c-819f-4d41c6b8fdd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Similarity vs. Visual Similarity:\n",
      "\trho     = 0.1212\n",
      "\tp-value = 0.0007\n",
      "PPMI Similarity vs. Visual Similarity:\n",
      "\trho     = 0.3838\n",
      "\tp-value = 0.0000\n",
      "SVD Similarity vs. Visual Similarity:\n",
      "\trho     = 0.3097\n",
      "\tp-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Your code should go here...\n",
    "# for raw similarities\n",
    "rho_raw_vis, pval_raw_vis = stats.spearmanr(visual_similarity, raw_similarities)\n",
    "print(\"\"\"Raw Similarity vs. Visual Similarity:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_raw_vis, pval_raw_vis))\n",
    "# for PPMI similarities\n",
    "rho_ppmi_vis, pval_ppmi_vis = stats.spearmanr(visual_similarity, ppmi_similarities)\n",
    "print(\"\"\"PPMI Similarity vs. Visual Similarity:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_ppmi_vis, pval_ppmi_vis))\n",
    "# for SVD similarities\n",
    "rho_svd_vis, pval_svd_vis = stats.spearmanr(visual_similarity, svd_similarities)\n",
    "print(\"\"\"SVD Similarity vs. Visual Similarity:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_svd_vis, pval_svd_vis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mevtF-TcfvFo"
   },
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "For visual similarity, it's also the PPMI model which correlates best.\n",
    "\n",
    "The correlation coefficients obtained with the real visual similarity are lower than those obtained with the real semantic similarity, for each model. It is to be noticed that for each model, coefficients obtained for visual and semantic similarities are quite close, and the variation from model to model is close for visual and semantic similarities. It may show an existing relationship betwwen semantic and visual similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZxBXRZMfvFo"
   },
   "source": [
    "## 4. Operations on similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx6aCkHOfvFo"
   },
   "source": [
    "We can perform mathematical operations on vectors to derive meaning predictions.\n",
    "\n",
    "For example, we can perform `king - man` and add the resulting vector to `woman` and we hope to get the vector for `queen`. What would be the result of `stockholm - sweden + denmark`? Why? **[3 marks]**\n",
    "\n",
    "If you want to learn more about vector differences between words (and words in analogy relations), check this paper [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFZj7LpEfvFo"
   },
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "stockholm - sweden + denmark should result in copenhagen because the vector stockholm - sweden shoud represent the meaning of capital city. So combined with denmark it should give the danish capital city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "94VV9YYefvFo",
    "outputId": "9bb1ed53-ce62-4e6d-d668-c56cb26205ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03552611 0.545724   1.0181931  ... 0.         0.         0.        ]\n",
      "[0.         0.32761374 0.14667151 ... 0.         1.65460267 0.        ]\n",
      "[0.         0.24733743 0.         ... 0.         0.         0.        ]\n",
      "[0.07510489 0.53714324 0.81981624 ... 0.         0.         0.        ]\n",
      "[ 0.03552611  0.4654477   0.87152159 ...  0.         -1.65460267\n",
      "  0.        ]\n",
      "[0.07510489 0.53714324 0.81981624 ... 0.         0.         0.        ]\n",
      "Cosine similarity betwwen 'queen' and 'king' - 'man' + 'woman': 0.24685292900604752\n",
      "[ 0.77140793  0.         -0.30190277 ...  0.          0.\n",
      "  0.        ]\n",
      "[0.43039703 0.         0.3714493  ... 0.         0.         0.        ]\n",
      "Cosine similarity betwwen 'copenhagen' and 'stockholm' - 'sweden' + 'denmark': 0.10368265641034352\n"
     ]
    }
   ],
   "source": [
    "vec1 = ppmispace_10k['king'] - ppmispace_10k['man'] + ppmispace_10k['woman']\n",
    "vec2 = ppmispace_10k['queen']\n",
    "print(ppmispace_10k['king'])\n",
    "print(ppmispace_10k['man'])\n",
    "print(ppmispace_10k['woman'])\n",
    "print(ppmispace_10k['queen'])\n",
    "print(vec1)\n",
    "print(vec2)\n",
    "print(\"Cosine similarity betwwen 'queen' and 'king' - 'man' + 'woman':\", np.sum(vec1 * vec2) / (veclen(vec1) * veclen(vec2))) #cosine similarity vec1 / vec2\n",
    "\n",
    "\n",
    "vec3 = ppmispace_10k['stockholm'] - ppmispace_10k['sweden'] + ppmispace_10k['denmark']\n",
    "vec4 = ppmispace_10k['copenhagen']\n",
    "print(vec3)\n",
    "print(vec4)\n",
    "print(\"Cosine similarity betwwen 'copenhagen' and 'stockholm' - 'sweden' + 'denmark':\", np.sum(vec3 * vec4) / (veclen(vec3) * veclen(vec4))) #cosine similarity vec3 / vec4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myQxcAhjfvFp"
   },
   "source": [
    "Here is some code that allows us to calculate such comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_pcjs2S3fvFp"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def normalize(vec):\n",
    "    return vec / veclen(vec)\n",
    "\n",
    "def find_similar_to(vec1, space):\n",
    "    # vector similarity funciton\n",
    "    #sim_fn = lambda a, b: 1-distance.euclidean(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.correlation(a, b)\n",
    "    #sim_fn = lambda a, b: 1-distance.cityblock(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.chebyshev(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: np.dot(normalize(a), normalize(b))\n",
    "    sim_fn = lambda a, b: 1-distance.cosine(a, b)\n",
    "\n",
    "    sims = [\n",
    "        (word2, sim_fn(vec1, space[word2]))\n",
    "        for word2 in space.keys()\n",
    "    ]\n",
    "    return sorted(sims, key = lambda p:p[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0oT9KdtfvFp"
   },
   "source": [
    "Here is how you apply this code. Comment on the results you get. **[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "l5pvRmBEfvFp",
    "outputId": "9b5e8a59-c773-4344-b351-4d5149b24d18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long', np.float64(0.8733111261346902)),\n",
       " ('above', np.float64(0.8259671977311956)),\n",
       " ('around', np.float64(0.8030776291120686)),\n",
       " ('sun', np.float64(0.7692439111243974)),\n",
       " ('just', np.float64(0.767848197477811)),\n",
       " ('wide', np.float64(0.7672574319922534)),\n",
       " ('each', np.float64(0.7665960260861158)),\n",
       " ('circle', np.float64(0.7647746702909335)),\n",
       " ('length', np.float64(0.7601066921319761)),\n",
       " ('almost', np.float64(0.7542351860536627))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = normalize(svdspace_10k['short'])\n",
    "light = normalize(svdspace_10k['light'])\n",
    "long = normalize(svdspace_10k['long'])\n",
    "heavy = normalize(svdspace_10k['heavy'])\n",
    "\n",
    "find_similar_to(light - (heavy - long), svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5-uHbOnmfvFp",
    "outputId": "49aef7bb-61a5-400f-e686-72138da9bc28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', np.float64(0.6491443577785698)),\n",
       " ('woman', np.float64(0.5197809644507281)),\n",
       " ('queen', np.float64(0.24465515993898057)),\n",
       " ('prince', np.float64(0.2401281327079562)),\n",
       " ('emperor', np.float64(0.23675776346331323)),\n",
       " ('ii', np.float64(0.22459826786171)),\n",
       " ('son', np.float64(0.21607422141963617)),\n",
       " ('iii', np.float64(0.21333759857917067)),\n",
       " ('louis', np.float64(0.20969687973075513)),\n",
       " ('charles', np.float64(0.2058020637334398))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queen = normalize(ppmispace_10k['queen'])\n",
    "king = normalize(ppmispace_10k['king'])\n",
    "man = normalize(ppmispace_10k['man'])\n",
    "woman = normalize(ppmispace_10k['woman'])\n",
    "find_similar_to(king - man + woman, ppmispace_10k)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sq303aiIfvFp",
    "outputId": "dd20245d-ef91-46bf-ba89-58029992009c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stockholm', np.float64(0.6471741063767443)),\n",
       " ('denmark', np.float64(0.5270482981264609)),\n",
       " ('paris', np.float64(0.14192180704346102)),\n",
       " ('prague', np.float64(0.13330803087198895)),\n",
       " ('moscow', np.float64(0.12723873038417588)),\n",
       " ('london', np.float64(0.12179613535983802)),\n",
       " ('berlin', np.float64(0.11239954334145585)),\n",
       " ('munich', np.float64(0.11111614973189443)),\n",
       " ('oslo', np.float64(0.10943906436877393)),\n",
       " ('vienna', np.float64(0.10922784597519919)),\n",
       " ('copenhagen', np.float64(0.10702413228724272)),\n",
       " ('montreal', np.float64(0.10121429569043294)),\n",
       " ('philadelphia', np.float64(0.10036811707045457)),\n",
       " ('toronto', np.float64(0.09830080285008835)),\n",
       " ('1876', np.float64(0.09723346442225633))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denmark = normalize(ppmispace_10k['denmark'])\n",
    "stockholm = normalize(ppmispace_10k['stockholm'])\n",
    "sweden = normalize(ppmispace_10k['sweden'])\n",
    "copenhagen = normalize(ppmispace_10k['copenhagen'])\n",
    "find_similar_to(stockholm - sweden + denmark, ppmispace_10k)[:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4z0_QrDfvFp"
   },
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "For light - heavy + long we don't get the expected result 'short' : maybe the model is confused due to 'light' and 'heavy' having several different meanings. We don't get 'short' but we get words associated with space and dimension or localisation (above, length, around), 'sun' is a surprising result here but it might be related to the second meaning of 'light'.\n",
    "\n",
    "For stockholm - sweden + denmark, copenhagen arrives late in the similar vectors list, but the list mainly includes capital cities which shows that the model performs well.\n",
    "\n",
    "For king - man + woman, queen is the third result in the list, which is fine. Besides the list contains words related to rulers (emperoor, prince, louis...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5KmHYaofvFp"
   },
   "source": [
    "Find 5 similar pairs of pairs of words and test them. Hint: google for `word analogies examples`. You can also construct analogies that are not only lexical but also express other relations such as grammatical relations, e.g. `see, saw, leave, ?` or analogies that are based on world knowledge as in `question-words.txt` from the [Google analogy dataset](http://download.tensorflow.org/data/questions-words.txt) described in [3]. Does the resulting vector similarity confirm your expectations? Remember you can only do this test if the words are contained in our vector space with 10,000 dimensions. **[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4h-lF2vHfvFq",
    "outputId": "9a871587-6a42-4f3b-e8ac-5466bbd418ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('leave', np.float64(0.7717628100683235)),\n",
       " ('stay', np.float64(0.7701972693040484)),\n",
       " ('abandon', np.float64(0.7593780673146007)),\n",
       " ('move', np.float64(0.7573350540949787)),\n",
       " ('shut', np.float64(0.755817876374274)),\n",
       " ('resign', np.float64(0.7502871358861033)),\n",
       " ('return', np.float64(0.7482934387903277)),\n",
       " ('meet', np.float64(0.7471493847747047)),\n",
       " ('put', np.float64(0.7469297945261502)),\n",
       " ('lose', np.float64(0.742779025865288)),\n",
       " ('stand', np.float64(0.741304842113655)),\n",
       " ('break', np.float64(0.7398152892159425)),\n",
       " ('bring', np.float64(0.7334528313285422)),\n",
       " ('fight', np.float64(0.7301413971995372)),\n",
       " ('lay', np.float64(0.7285960910279293))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code should go here...\n",
    "see = normalize(svdspace_10k['see'])\n",
    "saw = normalize(svdspace_10k['saw'])\n",
    "leave = normalize(svdspace_10k['leave'])\n",
    "left = normalize(svdspace_10k['left'])\n",
    "\n",
    "find_similar_to(saw - (see - leave), svdspace_10k)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "COlYYd5_fvFq",
    "outputId": "c3f486d0-62ca-40aa-a7c2-2d32d86b5d62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('works', np.float64(0.7682132682899971)),\n",
       " ('career', np.float64(0.7647854220454858)),\n",
       " ('work', np.float64(0.7363374544705507)),\n",
       " ('worked', np.float64(0.7356698664459044)),\n",
       " ('appeared', np.float64(0.7315200520007924)),\n",
       " ('began', np.float64(0.7243143729711581)),\n",
       " ('wrote', np.float64(0.7186745926162428)),\n",
       " ('became', np.float64(0.7167849898531065)),\n",
       " ('went', np.float64(0.7120800159218981)),\n",
       " ('started', np.float64(0.7055818131919861)),\n",
       " ('met', np.float64(0.6950394491706384)),\n",
       " ('role', np.float64(0.6916456155990831)),\n",
       " ('took', np.float64(0.6882249479502374)),\n",
       " ('era', np.float64(0.6840303739142773)),\n",
       " ('followed', np.float64(0.6812720145732776))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code should go here...\n",
    "go = normalize(svdspace_10k['go'])\n",
    "went = normalize(svdspace_10k['went'])\n",
    "work = normalize(svdspace_10k['work'])\n",
    "worked = normalize(svdspace_10k['worked'])\n",
    "\n",
    "find_similar_to(went - (go - work), svdspace_10k)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RW6s1KIfvFq"
   },
   "source": [
    "About the grammatical relation infinitive/preterit forms: at first with the first test we concluded that the model can't perform well due to the stemization as it is mentioned in comment in the library dist_erk.py. However, looking into the code in more details we noticed that our models actually don't work with stems only, and we got confused. After some discussion between us and with Mattias, we decided to do the same test with some other words which aren't ambiguous. And the second test shows that the model actually performs well, as the expected result 'worked' is returned. As a conclusion, for the first example, the model was certainly confused with the different meanings of 'saw' and 'left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "r_H9Meg_fvFq",
    "outputId": "ac4da53f-fb08-4b14-cbb9-08b50f72f7e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('safely', np.float64(0.8367927222953683)),\n",
       " ('drop', np.float64(0.7724152355781814)),\n",
       " ('slowly', np.float64(0.7584178211700946)),\n",
       " ('sink', np.float64(0.7328318296579761)),\n",
       " ('leg', np.float64(0.7315775451698037)),\n",
       " ('plane', np.float64(0.7278822154760428)),\n",
       " ('intentionally', np.float64(0.7276022163631448)),\n",
       " ('suddenly', np.float64(0.7255999410004005)),\n",
       " ('fires', np.float64(0.7172154718762171)),\n",
       " ('discharge', np.float64(0.716919892696525))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code should go here...\n",
    "safe = normalize(svdspace_10k['safe'])\n",
    "safely = normalize(svdspace_10k['safely'])\n",
    "slow = normalize(svdspace_10k['slow'])\n",
    "slowly = normalize(svdspace_10k['slowly'])\n",
    "\n",
    "find_similar_to(safely - (safe - slow), svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyR6-7O2fvFq"
   },
   "source": [
    "About adverbs/adjectives forms relation: the model performs quite well as we get the expected result 'slowly' in third position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "75tDSgOJfvFq",
    "outputId": "845ee5da-a350-4fc2-803f-9c209ee5d282"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unclear', np.float64(0.8019858714685726)),\n",
       " ('unlikely', np.float64(0.7635318354914318)),\n",
       " ('likely', np.float64(0.7587057038069621)),\n",
       " ('uncertain', np.float64(0.7531067951102806)),\n",
       " ('fatal', np.float64(0.7472101357720266)),\n",
       " ('possible', np.float64(0.737763108554002)),\n",
       " ('inevitable', np.float64(0.7344989416834508)),\n",
       " ('probable', np.float64(0.7323442226054725)),\n",
       " ('incorrect', np.float64(0.726845381672453)),\n",
       " ('autism', np.float64(0.7265489567003465)),\n",
       " ('worse', np.float64(0.7193373085385575)),\n",
       " ('beneficial', np.float64(0.7148626113260488)),\n",
       " ('obvious', np.float64(0.7055427617364421)),\n",
       " ('documented', np.float64(0.7026494233525161)),\n",
       " ('hiv', np.float64(0.7025376156186143))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code should go here...\n",
    "clear = normalize(svdspace_10k['clear'])\n",
    "unclear = normalize(svdspace_10k['unclear'])\n",
    "possible = normalize(svdspace_10k['possible'])\n",
    "impossible = normalize(svdspace_10k['impossible'])\n",
    "\n",
    "find_similar_to(unclear - (clear - possible), svdspace_10k)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mWi1Hq8qfvFq",
    "outputId": "88008c7d-00c5-4e2a-bead-4ae1ee07351f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unclear', np.float64(0.7417766270923434)),\n",
       " ('certain', np.float64(0.740289056174162)),\n",
       " ('uncertain', np.float64(0.7372605182197638)),\n",
       " ('exist', np.float64(0.7300752063052249)),\n",
       " ('differing', np.float64(0.7229690702234336)),\n",
       " ('related', np.float64(0.7123033756302716)),\n",
       " ('common', np.float64(0.7099805095087283)),\n",
       " ('associated', np.float64(0.7079283489014209)),\n",
       " ('beneficial', np.float64(0.7066936210552571)),\n",
       " ('everyday', np.float64(0.7005789816594914)),\n",
       " ('unrelated', np.float64(0.6999577323586899)),\n",
       " ('vary', np.float64(0.6993012080198612)),\n",
       " ('arise', np.float64(0.6986075346912508)),\n",
       " ('these', np.float64(0.698591241492787)),\n",
       " ('autism', np.float64(0.6981623819043589))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code should go here...\n",
    "clear = normalize(svdspace_10k['clear'])\n",
    "unclear = normalize(svdspace_10k['unclear'])\n",
    "certain = normalize(svdspace_10k['certain'])\n",
    "uncertain = normalize(svdspace_10k['uncertain'])\n",
    "\n",
    "find_similar_to(unclear - (clear - certain), svdspace_10k)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZXzE4qEfvFq"
   },
   "source": [
    "About adjectives antonyms: testing different prefixes show that the model performs better in recognizing antonyms built with the same prefix than antonyms built with different prefixes ('impossible' is not obtained from 'clear' and 'unclear', whereas 'uncertain' appears from the same words pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YHQhLDbxfvFq",
    "outputId": "614073c8-b7c4-4072-91bc-564cacc34ccb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bigger', np.float64(0.7914575619512246)),\n",
       " ('worse', np.float64(0.7553937803306197)),\n",
       " ('looking', np.float64(0.7269805214567301)),\n",
       " ('aging', np.float64(0.7239652963405067)),\n",
       " ('grows', np.float64(0.7234485733586011)),\n",
       " ('sharp', np.float64(0.7213339295938627)),\n",
       " ('big', np.float64(0.7208705963298278)),\n",
       " ('rising', np.float64(0.7164153233012861)),\n",
       " ('riding', np.float64(0.7144412898508123)),\n",
       " ('unfortunately', np.float64(0.711821020483255)),\n",
       " ('interestingly', np.float64(0.7098070541244442)),\n",
       " ('bug', np.float64(0.7094753414147278)),\n",
       " ('dropping', np.float64(0.7082867934036383)),\n",
       " ('gets', np.float64(0.7060384528744815)),\n",
       " ('broken', np.float64(0.7036461939988118))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code should go here...\n",
    "bad = normalize(svdspace_10k['bad'])\n",
    "worse = normalize(svdspace_10k['worse'])\n",
    "big = normalize(svdspace_10k['big'])\n",
    "bigger = normalize(svdspace_10k['bigger'])\n",
    "\n",
    "find_similar_to(worse - (bad - big), svdspace_10k)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfaGOnzKfvFq"
   },
   "source": [
    "About adjectives positive/comparative forms: the model performs as expected, 'bigger' being the first result in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JvogozowfvFq",
    "outputId": "5c5bf2e4-ef10-4634-e14c-125a1ee25da0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('insects', np.float64(0.8239032697493532)),\n",
       " ('rats', np.float64(0.8077913450941908)),\n",
       " ('birds', np.float64(0.8038784714162265)),\n",
       " ('ants', np.float64(0.7899042170335102)),\n",
       " ('whales', np.float64(0.7863088380070324)),\n",
       " ('animals', np.float64(0.7778530868904784)),\n",
       " ('bird', np.float64(0.7762879802801479)),\n",
       " ('mice', np.float64(0.7753622048372169)),\n",
       " ('mammals', np.float64(0.771623363038249)),\n",
       " ('whale', np.float64(0.7510836335792803)),\n",
       " ('bees', np.float64(0.7477381760454581)),\n",
       " ('bats', np.float64(0.7453179443078095)),\n",
       " ('vegetation', np.float64(0.7293494912806123)),\n",
       " ('dogs', np.float64(0.7217067271991996)),\n",
       " ('pigs', np.float64(0.7152532509202817))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code should go here...\n",
    "mouse = normalize(svdspace_10k['mouse'])\n",
    "mice = normalize(svdspace_10k['mice'])\n",
    "bird = normalize(svdspace_10k['bird'])\n",
    "birds = normalize(svdspace_10k['birds'])\n",
    "\n",
    "find_similar_to(mice - (mouse - bird), svdspace_10k)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXCHPeVvfvFr"
   },
   "source": [
    "About plural/singular forms for nouns: we get the expected result 'birds' as the third result, which is rather good. Also we can notice that most of the returned vectors are animals in plural forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YLssIqtfvFr"
   },
   "source": [
    "General comment: Mathematical operations on vectors to test meaning predictions is not perfect but considering the size of our model we still can say it performs quite well, as we can see some consistency between the expectations and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZc3G0V8fvFr"
   },
   "source": [
    "## 5. Semantic composition and phrase similarity **[20 marks]**\n",
    "\n",
    "In this task, we are going to examine how the composed vectors of phrases by different semantic composition functions/models introduced in [2] correlate with human judgements of similarity between phrases. We will use the dataset from this paper which is stored in `mitchell_lapata_acl08.txt`. If you are interested about further details about this task also refer to this paper.\n",
    "\n",
    "(i) Process the dataset. The dataset contains human judgemements of similarity between phrases recorded one per line. The first column indicates the id of a participant making a judgement (`participant`), the next column is `verb`, followed by `noun` and `landmark`. From these three columns we can construct phrases that were compared by human informants, namely `verb noun` vs `verb landmark`. The next column `input` indicates a similarity score a participant assigned to a pair of such phrases on a scale from 1 to 7 where 1 is lowest and 7 is highest. The last column `hilo` groups the phrases into two sets: phrases where we expect low and phrases where we expect high similarity scores. This is because we want to test our compositional functions on two tasks and examine whether a function is discriminative between them. Correlation between scores could also be due to other reasons than semantic similarity and hence good prediction on both tasks simultaneously shows that a function is truly discriminating the phrases using some semantic criteria.\n",
    "\n",
    "For extracting information you can use the code from the lecture to start with. How to structure this data is up to you - a dictionary-like format would be a good choice. Remember that each example was judged by several participants and phrases will repeat in the dataset. Therefore, you have to collect all judgments for a particular set of phrases and average them. This will become useful in step (iii).\n",
    "\n",
    "(ii) Compose the vectors of the extracted word pairs by testing different compositional functions. In the lecture we introduced simple additive, simple multiplicative and combined models (details are described in [2]). Your task is to take a pair of phrases, e.g. the first example in the dataset `stray thought` and `stray roam` and for each phrase compute a composition of the vectors of their words using these functions, using one function per experiment run. For each phrase you will get a single vector. You can encode the words with any vector space introduced earlier (standard space, ppmi or svd) but your code should be structured in a way that it will be easy to switch between them. Finally, take the resulting (composed) vectors of phrase pairs in the dataset and calculate a cosine similarity between them.\n",
    "\n",
    "(iii) Now you have cosine similairity scores between vectors of phrases but how do they compare with the average human scores that you calculated from the individual judgements from the `input` column of the dataset for the same phrases? Calculate Spearman rank correlation coefficient between two lists of the scores both for the `high` and the `low` task .\n",
    "\n",
    "We use the Spearmank rank correlation coefficient (or Spearman's rho) rather than Peason's correlation coefficent because we cannot compare cosine scores with human judgements directly. Cosine is a constinuous measure and human judgements are expressed as ranks. Also, we cannot say if 0.28 to 1 is the same (or different) to 6 to 7 in the human scores.  The Spearman rank correlation coeffcient turns the scores for all examples within each group first to ranks and then these ranks are correlated (or approximated to a linear function).\n",
    "\n",
    "In the end you should get a table similar to the one below from the paper. What is the best compositional function from those that you evaluated with your vector spaces and why?\n",
    "\n",
    "<img src=\"res.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Note that you might not get results in the same range as those in the paper.\n",
    "That is ok, a good interpretation of results and discussion why sometimes they are not as good as you would expect is better than giving the best performing results with little to no analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jUL-Rt6-fvFr",
    "outputId": "c7e6d347-cf75-40f4-9db5-80498b4dd247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant verb noun landmark input hilo\n",
      "participant20 stray thought roam 7 low\n",
      "participant20 stray discussion digress 6 high\n",
      "participant20 stray eye roam 7 high\n",
      "participant20 stray child digress 1 low\n",
      "participant20 throb body pulse 5 high\n",
      "participant20 throb head shudder 2 low\n",
      "participant20 throb voice shudder 3 low\n",
      "participant20 throb vein pulse 6 high\n",
      "participant20 chatter machine click 4 high\n",
      "stray\n",
      "roam\n",
      "digress\n",
      "throb\n",
      "pulse\n",
      "shudder\n",
      "vein\n",
      "chatter\n",
      "gabble\n",
      "tooth\n",
      "rebound\n",
      "ricochet\n",
      "optimism\n",
      "flicker\n",
      "waver\n",
      "flick\n",
      "subside\n",
      "lessen\n",
      "symptom\n",
      "slump\n",
      "slouch\n",
      "stoop\n",
      "erupt\n",
      "burst\n",
      "temper\n",
      "flare\n",
      "recoil\n",
      "flinch\n",
      "prosper\n",
      "fluctuate\n",
      "falter\n",
      "cigarette\n",
      "reel\n",
      "whirl\n",
      "stagger\n",
      "glow\n",
      "cigar\n",
      "95 58\n",
      "{('bow', 'butler', 'submit'): {'input': [3, 2, 2, 3, 4, 5, 4, 2, 2, 2, 3, 5, 4, 2, 2, 4, 6, 3, 7, 1, 2, 2, 5, 2, 2, 1, 2, 3, 3, 1, 1, 1, 4, 5], 'hilo': 'low'}, ('bow', 'company', 'submit'): {'input': [5, 6, 6, 4, 6, 5, 5, 4, 6, 2, 2, 5, 6, 6, 2, 4, 4, 6, 7, 6, 6, 2, 3, 3, 2, 4, 2, 5, 6, 2, 5, 2, 5, 3], 'hilo': 'high'}, ('boom', 'sale', 'thunder'): {'input': [3, 5, 2, 3, 1, 2, 2, 2, 4, 5, 3, 2, 4, 1, 3, 2, 3, 4, 6, 5, 2, 2, 1, 3, 1, 1, 2, 2, 2, 1, 6, 3, 4, 3], 'hilo': 'low'}, ('boom', 'gun', 'thunder'): {'input': [6, 6, 7, 5, 6, 6, 5, 6, 6, 6, 6, 6, 6, 7, 4, 6, 7, 6, 7, 5, 7, 6, 3, 6, 4, 3, 3, 6, 6, 7, 5, 6, 7, 3], 'hilo': 'high'}, ('bow', 'head', 'submit'): {'input': [3, 4, 3, 3, 5, 2, 5, 5, 1, 2, 3, 6, 2, 4, 4, 4, 1, 1, 1, 3, 3, 4, 5, 2, 7, 2], 'hilo': 'low'}, ('bow', 'government', 'submit'): {'input': [6, 6, 3, 7, 7, 5, 3, 2, 5, 5, 6, 7, 6, 6, 7, 6, 3, 7, 4, 6, 3, 2, 7, 7, 7, 7], 'hilo': 'high'}, ('boom', 'noise', 'thunder'): {'input': [6, 6, 6, 7, 7, 7, 6, 6, 6, 5, 5, 6, 7, 7, 6, 7, 6, 7, 5, 7, 6, 7, 5, 6, 7, 3], 'hilo': 'high'}, ('boom', 'export', 'thunder'): {'input': [2, 4, 3, 5, 2, 3, 5, 3, 2, 1, 4, 2, 1, 2, 1, 2, 1, 1, 4, 6, 1, 3, 2, 3, 5, 4], 'hilo': 'low'}}\n",
      "{('bow', 'butler', 'submit'): {'average input': 2.9411764705882355, 'hilo': 'low'}, ('bow', 'company', 'submit'): {'average input': 4.323529411764706, 'hilo': 'high'}, ('boom', 'sale', 'thunder'): {'average input': 2.7941176470588234, 'hilo': 'low'}, ('boom', 'gun', 'thunder'): {'average input': 5.617647058823529, 'hilo': 'high'}, ('bow', 'head', 'submit'): {'average input': 3.269230769230769, 'hilo': 'low'}, ('bow', 'government', 'submit'): {'average input': 5.384615384615385, 'hilo': 'high'}, ('boom', 'noise', 'thunder'): {'average input': 6.115384615384615, 'hilo': 'high'}, ('boom', 'export', 'thunder'): {'average input': 2.769230769230769, 'hilo': 'low'}}\n"
     ]
    }
   ],
   "source": [
    "# (i) - Process the data\n",
    "# your code should go here\n",
    "## this part of code is from the example provided\n",
    "# load the task dataset\n",
    "with open('mitchell_lapata_acl08.txt', 'r') as f:\n",
    "    phrase_dataset = f.read().splitlines()\n",
    "\n",
    "for line in phrase_dataset[:10]:\n",
    "    print(line)\n",
    "\n",
    "# get all unique words\n",
    "words = []\n",
    "for line in phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in words:\n",
    "        words.append(verb)\n",
    "    if noun not in words:\n",
    "        words.append(noun)\n",
    "    if landmark not in words:\n",
    "        words.append(landmark)\n",
    "\n",
    "# are there any words in the task dataset which do not appear in the reference corpus?\n",
    "to_remove = []\n",
    "for w in words:\n",
    "    if w not in svdspace_10k.keys():\n",
    "        print(w)\n",
    "        to_remove.append(w)\n",
    "# if some words are not found in the reference corpus, makes sense to ignore whole phrases with such words\n",
    "\n",
    "# how many words does our task dataset has in general? and without words to remove?\n",
    "print(len(words), len(words) - len(to_remove))\n",
    "\n",
    "# pre-processing the task dataset\n",
    "# we are removing all phrases which contain words which are not in the reference corpus\n",
    "preprocessed_phrase_dataset = []\n",
    "for line in phrase_dataset:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb in to_remove or noun in to_remove or landmark in to_remove:\n",
    "        continue\n",
    "    preprocessed_phrase_dataset.append(line)\n",
    "\n",
    "target_words = []\n",
    "for line in preprocessed_phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in target_words:\n",
    "        target_words.append(verb)\n",
    "    if noun not in target_words:\n",
    "        target_words.append(noun)\n",
    "    if landmark not in target_words:\n",
    "        target_words.append(landmark)\n",
    "# how many words do we have after pre-processing\n",
    "len(target_words)\n",
    "\n",
    "## this part is our code\n",
    "# get average human judgements for each set of phrases\n",
    "processed_phrase_dataset = {}\n",
    "temp_phrase_dataset = {}\n",
    "for line in preprocessed_phrase_dataset[1:]:\n",
    "    participant, verb, noun, landmark, input, hilo = line.split()\n",
    "    if (verb, noun, landmark) not in temp_phrase_dataset.keys():\n",
    "        temp_phrase_dataset[(verb, noun, landmark)] = {'input': [], 'hilo': hilo}\n",
    "    temp_phrase_dataset[(verb, noun, landmark)]['input'].append(int(input))\n",
    "print(temp_phrase_dataset)\n",
    "for (verb, noun, landmark) in temp_phrase_dataset.keys():\n",
    "    average_input = sum([input for input in temp_phrase_dataset[(verb, noun, landmark)]['input']]) / len(temp_phrase_dataset[(verb, noun, landmark)]['input'])\n",
    "    processed_phrase_dataset[(verb, noun, landmark)] = {'average input': average_input, 'hilo' : temp_phrase_dataset[(verb, noun, landmark)]['hilo']}\n",
    "print(processed_phrase_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "R1NHpS3tfvFr"
   },
   "outputs": [],
   "source": [
    "# (ii) - Compose the vectors of the extracted word pairs by testing different compositional functions\n",
    "# your code should go here\n",
    "\n",
    "# simple additive compositional function\n",
    "def add_comp_function(word1, word2, space, alpha, beta, gamma):\n",
    "    return space[word1] + space[word2]\n",
    "\n",
    "# simple multiplicative compositional function\n",
    "def mult_comp_function(word1, word2, space, alpha, beta, gamma):\n",
    "    return space[word1] * space[word2]\n",
    "\n",
    "# combined model for compositional function\n",
    "def comb_comp_function(word1, word2, space, alpha, beta, gamma):\n",
    "    return alpha * space[word1] + beta * space[word2] + gamma * space[word1] * space[word2]\n",
    "\n",
    "# function to compose the vectors, with the compositional function as a parameter\n",
    "def compose_vectors(comp_function, space, alpha, beta, gamma):\n",
    "    similarities = {}\n",
    "    for (verb, noun, landmark) in processed_phrase_dataset.keys():\n",
    "        similarities[(verb, noun, landmark)] = {}\n",
    "        vec_comp_noun = comp_function(verb, noun, space, alpha, beta, gamma)\n",
    "        vec_comp_landmark = comp_function(verb, landmark, space, alpha, beta, gamma)\n",
    "        similarities[(verb, noun, landmark)]['average input'] = processed_phrase_dataset[(verb, noun, landmark)]['average input']\n",
    "        similarities[(verb, noun, landmark)]['hilo'] = processed_phrase_dataset[(verb, noun, landmark)]['hilo']\n",
    "        similarities[(verb, noun, landmark)]['comp vector noun'] = vec_comp_noun\n",
    "        similarities[(verb, noun, landmark)]['comp vector landmark'] = vec_comp_landmark\n",
    "        similarities[(verb, noun, landmark)]['comp vectors cosine similarity'] = np.sum(vec_comp_noun * vec_comp_landmark) / (veclen(vec_comp_noun) * veclen(vec_comp_landmark))\n",
    "    return similarities\n",
    "\n",
    "# composing the vectors using the simple additive compositional function and the svdspace_10k\n",
    "processed_phrase_dataset_add = compose_vectors(add_comp_function, svdspace_10k, 0, 0, 0)\n",
    "\n",
    "# composing the vectors using the simple multiplicative compositional function and the svdspace_10k\n",
    "processed_phrase_dataset_mult = compose_vectors(mult_comp_function, svdspace_10k, 0, 0, 0)\n",
    "\n",
    "# composing the vectors using the combined compositional function with parameetrs as the paper and the svdspace_10k\n",
    "processed_phrase_dataset_comb = compose_vectors(comb_comp_function, svdspace_10k, 0.95, 0, 0.05)\n",
    "\n",
    "# composing the vectors using the combined compositional function  but switching the values for alpha and beta (as we build a VP and not a sentence in our case) and the svdspace_10k\n",
    "processed_phrase_dataset_comb_rev = compose_vectors(comb_comp_function, svdspace_10k, 0, 0.95, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sAPiVy0bfvFr",
    "outputId": "fd837484-9dc5-4817-9546-6fdc46a603c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the simple additive function\n",
      "Cosine Similarity vs. Average human score for the high task:\n",
      "\trho     = 1.0000\n",
      "\tp-value = 0.0000\n",
      "Cosine Similarity vs. Average human score for the low task:\n",
      "\trho     = -0.6000\n",
      "\tp-value = 0.4000\n",
      "For the simple multiplicative function\n",
      "Cosine Similarity vs. Average human score for the high task:\n",
      "\trho     = 1.0000\n",
      "\tp-value = 0.0000\n",
      "Cosine Similarity vs. Average human score for the low task:\n",
      "\trho     = -0.8000\n",
      "\tp-value = 0.2000\n",
      "For the combined function with parameters as in the paper\n",
      "Cosine Similarity vs. Average human score for the high task:\n",
      "\trho     = 1.0000\n",
      "\tp-value = 0.0000\n",
      "Cosine Similarity vs. Average human score for the low task:\n",
      "\trho     = -0.4000\n",
      "\tp-value = 0.6000\n",
      "For the combined function with parameters alpha and beta switched compared to thethe paper\n",
      "Cosine Similarity vs. Average human score for the high task:\n",
      "\trho     = 1.0000\n",
      "\tp-value = 0.0000\n",
      "Cosine Similarity vs. Average human score for the low task:\n",
      "\trho     = 0.8000\n",
      "\tp-value = 0.2000\n"
     ]
    }
   ],
   "source": [
    "# (iii) - Compare the cosine similarity scores between vectors of phrases with the average human scores\n",
    "# your code should go here\n",
    "\n",
    "sorted_keys = sorted(list(processed_phrase_dataset.keys()))\n",
    "\n",
    "# building the lists of human scores for each task\n",
    "average_human_similarity_high = [processed_phrase_dataset[(verb, noun, landmark)]['average input']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset[(verb, noun, landmark)]['hilo'] == 'high']\n",
    "average_human_similarity_low = [processed_phrase_dataset[(verb, noun, landmark)]['average input']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset[(verb, noun, landmark)]['hilo'] == 'low']\n",
    "\n",
    "# Using simple additive compositional function to build the list of similarities for each task\n",
    "\n",
    "comp_vectors_similarity_add_high = [processed_phrase_dataset_add[(verb, noun, landmark)]['comp vectors cosine similarity']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset_add[(verb, noun, landmark)]['hilo'] == 'high']\n",
    "comp_vectors_similarity_add_low = [processed_phrase_dataset_add[(verb, noun, landmark)]['comp vectors cosine similarity']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset_add[(verb, noun, landmark)]['hilo'] == 'low']\n",
    "\n",
    "rho_add_high, pval_add_high = stats.spearmanr(average_human_similarity_high, comp_vectors_similarity_add_high)\n",
    "print('For the simple additive function')\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the high task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_add_high, pval_add_high))\n",
    "\n",
    "rho_add_low, pval_add_low = stats.spearmanr(average_human_similarity_low, comp_vectors_similarity_add_low)\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the low task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_add_low, pval_add_low))\n",
    "\n",
    "# Using simple multiplicative compositional function to build the list of similarities for each task\n",
    "\n",
    "comp_vectors_similarity_mult_high = [processed_phrase_dataset_mult[(verb, noun, landmark)]['comp vectors cosine similarity']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset_mult[(verb, noun, landmark)]['hilo'] == 'high']\n",
    "comp_vectors_similarity_mult_low = [processed_phrase_dataset_mult[(verb, noun, landmark)]['comp vectors cosine similarity']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset_mult[(verb, noun, landmark)]['hilo'] == 'low']\n",
    "\n",
    "rho_mult_high, pval_mult_high = stats.spearmanr(average_human_similarity_high, comp_vectors_similarity_mult_high)\n",
    "print('For the simple multiplicative function')\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the high task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_mult_high, pval_mult_high))\n",
    "\n",
    "rho_mult_low, pval_mult_low = stats.spearmanr(average_human_similarity_low, comp_vectors_similarity_mult_low)\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the low task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_mult_low, pval_mult_low))\n",
    "\n",
    "# Using combined compositional function with parameters as in the paper to build the list of similarities for each task\n",
    "\n",
    "comp_vectors_similarity_comb_high = [processed_phrase_dataset_comb[(verb, noun, landmark)]['comp vectors cosine similarity']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset_comb[(verb, noun, landmark)]['hilo'] == 'high']\n",
    "comp_vectors_similarity_comb_low = [processed_phrase_dataset_comb[(verb, noun, landmark)]['comp vectors cosine similarity']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset_comb[(verb, noun, landmark)]['hilo'] == 'low']\n",
    "\n",
    "rho_comb_high, pval_comb_high = stats.spearmanr(average_human_similarity_high, comp_vectors_similarity_comb_high)\n",
    "print('For the combined function with parameters as in the paper')\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the high task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_comb_high, pval_comb_high))\n",
    "\n",
    "rho_comb_low, pval_comb_low = stats.spearmanr(average_human_similarity_low, comp_vectors_similarity_comb_low)\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the low task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_comb_low, pval_comb_low))\n",
    "\n",
    "# Using combined compositional function with parameters alpha and beta switched compared to the paper to build the list of similarities for each task\n",
    "\n",
    "comp_vectors_similarity_comb_rev_high = [processed_phrase_dataset_comb_rev[(verb, noun, landmark)]['comp vectors cosine similarity']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset_comb_rev[(verb, noun, landmark)]['hilo'] == 'high']\n",
    "comp_vectors_similarity_comb_rev_low = [processed_phrase_dataset_comb_rev[(verb, noun, landmark)]['comp vectors cosine similarity']\n",
    "                                 for (verb, noun, landmark) in sorted_keys if processed_phrase_dataset_comb_rev[(verb, noun, landmark)]['hilo'] == 'low']\n",
    "\n",
    "rho_comb_rev_high, pval_comb_rev_high = stats.spearmanr(average_human_similarity_high, comp_vectors_similarity_comb_rev_high)\n",
    "print('For the combined function with parameters alpha and beta switched compared to thethe paper')\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the high task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_comb_rev_high, pval_comb_rev_high))\n",
    "\n",
    "rho_comb_rev_low, pval_comb_rev_low = stats.spearmanr(average_human_similarity_low, comp_vectors_similarity_comb_rev_low)\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the low task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_comb_rev_low, pval_comb_rev_low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the simple additive function (whole task)\n",
      "Cosine Similarity vs. Average human score for the whole task:\n",
      "\trho     = -0.0238\n",
      "\tp-value = 0.9554\n",
      "For the simple multiplicative function (whole task)\n",
      "Cosine Similarity vs. Average human score for the whole task:\n",
      "\trho     = 0.2857\n",
      "\tp-value = 0.4927\n",
      "For the combined function with parameters as in the paper (whole task)\n",
      "Cosine Similarity vs. Average human score for the whole task:\n",
      "\trho     = -0.5952\n",
      "\tp-value = 0.1195\n",
      "For the combined function with parameters alpha and beta switched compared to the paper (whole task)\n",
      "Cosine Similarity vs. Average human score for the whole task:\n",
      "\trho     = 0.7381\n",
      "\tp-value = 0.0366\n"
     ]
    }
   ],
   "source": [
    "# calculate average similarity scores without segragating high and low tasks\n",
    "average_human_similarity_all = average_human_similarity_high + average_human_similarity_low\n",
    "\n",
    "comp_vectors_similarity_add_all = comp_vectors_similarity_add_high + comp_vectors_similarity_add_low\n",
    "comp_vectors_similarity_mult_all = comp_vectors_similarity_mult_high + comp_vectors_similarity_mult_low\n",
    "comp_vectors_similarity_comb_all = comp_vectors_similarity_comb_high + comp_vectors_similarity_comb_low\n",
    "comp_vectors_similarity_comb_rev_all = comp_vectors_similarity_comb_rev_high + comp_vectors_similarity_comb_rev_low\n",
    "\n",
    "rho_add_all, pval_add_all = stats.spearmanr(average_human_similarity_all, comp_vectors_similarity_add_all)\n",
    "print('For the simple additive function (whole task)')\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the whole task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_add_all, pval_add_all))\n",
    "\n",
    "rho_mult_all, pval_mult_all = stats.spearmanr(average_human_similarity_all, comp_vectors_similarity_mult_all)\n",
    "print('For the simple multiplicative function (whole task)')\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the whole task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_mult_all, pval_mult_all))\n",
    "\n",
    "rho_comb_all, pval_comb_all = stats.spearmanr(average_human_similarity_all, comp_vectors_similarity_comb_all)\n",
    "print('For the combined function with parameters as in the paper (whole task)')\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the whole task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_comb_all, pval_comb_all))\n",
    "\n",
    "rho_comb_rev_all, pval_comb_rev_all = stats.spearmanr(average_human_similarity_all, comp_vectors_similarity_comb_rev_all)\n",
    "print('For the combined function with parameters alpha and beta switched compared to the paper (whole task)')\n",
    "print(\"\"\"Cosine Similarity vs. Average human score for the whole task:\n",
    "\\trho     = {:.4f}\n",
    "\\tp-value = {:.4f}\"\"\".format(rho_comb_rev_all, pval_comb_rev_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model    High     Low     rho  p-value\n",
      "0        Additive  0.6996  0.7416 -0.0238   0.9554\n",
      "1  Multiplicative  0.9282  0.9328  0.2857   0.4927\n",
      "2        Combined  0.8783  0.9797 -0.5952   0.1195\n",
      "3    Combined Rev  0.1691 -0.0229  0.7381   0.0366\n"
     ]
    }
   ],
   "source": [
    "# building the table comparing results for all models\n",
    "import pandas as pd\n",
    "\n",
    "high_means = [\n",
    "    sum(comp_vectors_similarity_add_high) / len(comp_vectors_similarity_add_high),\n",
    "    sum(comp_vectors_similarity_mult_high) / len(comp_vectors_similarity_mult_high),\n",
    "    sum(comp_vectors_similarity_comb_high) / len(comp_vectors_similarity_comb_high),\n",
    "    sum(comp_vectors_similarity_comb_rev_high) / len(comp_vectors_similarity_comb_rev_high)\n",
    "]\n",
    "\n",
    "low_means = [\n",
    "    sum(comp_vectors_similarity_add_low) / len(comp_vectors_similarity_add_low),\n",
    "    sum(comp_vectors_similarity_mult_low) / len(comp_vectors_similarity_mult_low),\n",
    "    sum(comp_vectors_similarity_comb_low) / len(comp_vectors_similarity_comb_low),\n",
    "    sum(comp_vectors_similarity_comb_rev_low) / len(comp_vectors_similarity_comb_rev_low)\n",
    "]\n",
    "\n",
    "rho_vals = [rho_add_all, rho_mult_all, rho_comb_all, rho_comb_rev_all]\n",
    "p_vals = [pval_add_all, pval_mult_all, pval_comb_all, pval_comb_rev_all]\n",
    "\n",
    "table_data = {\n",
    "    \"Model\": [\"Additive\", \"Multiplicative\", \"Combined\", \"Combined Rev\"],\n",
    "    \"High\": high_means,\n",
    "    \"Low\": low_means,\n",
    "    \"rho\": rho_vals,\n",
    "    \"p-value\": p_vals\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(table_data)\n",
    "\n",
    "df[\"High\"] = df[\"High\"].round(4)\n",
    "df[\"Low\"] = df[\"Low\"].round(4)\n",
    "df[\"rho\"] = df[\"rho\"].round(4)\n",
    "df[\"p-value\"] = df[\"p-value\"].round(4)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm2V5COifvFr"
   },
   "source": [
    "**Any comments/thoughts should go here:**\n",
    "\n",
    "The combined function (reversed) showed the best overall performance, with the highest correlation to human judgments. This suggests that weighting the noun more heavily in the composition helps capture semantic similarity more effectively. It may be related to the difference between the datasets used in the paper and in our current study, as in the paper the composed vectors represent a sentence built from an NP (subject) and a VP, whereas here they represent a VP built from a V and an (object) NP. \n",
    "\n",
    "However this result needs to be nuanced as the Combined Reversed model performing best is also the only one getting a valid p-value, which makes comparison and results difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkxcmi0BfvFs"
   },
   "source": [
    "# Literature\n",
    "\n",
    "[1] C. Silberer and M. Lapata. Learning grounded meaning representations with autoencoders. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732, Baltimore, Maryland, USA, June 23–25 2014 2014. Association for Computational Linguistics.  \n",
    "\n",
    "[2] Mitchell, J., & Lapata, M. (2008). Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT (pp. 236–244). Association for Computational Linguistics.\n",
    "  \n",
    "[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "\n",
    "[4] E. Vylomova, L. Rimell, T. Cohn, and T. Baldwin. Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning. arXiv, arXiv:1509.01692 [cs.CL], 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91kjfsUCfvFs"
   },
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting.\n",
    "\n",
    "Before meeting, everyone of us worked individually on the assignment.\n",
    "We met once on the 24th April morning, around 2 hours, 3 of us on site and one online. We discussed our answers, mainly the part 5.\n",
    "\n",
    "We met again on the 28th to finalize our code and comments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOGwII61fvFs"
   },
   "source": [
    "## Marks\n",
    "\n",
    "The assignment is marked on a 7-level scale where 4 is sufficient to complete the assignment; 5 is good solid work; 6 is excellent work, covers most of the assignment; and 7: creative work.\n",
    "\n",
    "This assignment has a total of 60 marks. These translate to grades as follows: 1 = 17% 2 = 34%, 3 = 50%, 4 = 67%, 5 = 75%, 6 = 84%, 7 = 92% where %s are interpreted as lower bounds to achieve that grade."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
